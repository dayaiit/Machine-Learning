{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMymOHyYZb4gz+ap7yE4Ktk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dayaiit/Machine-Learning/blob/main/L2_Multiclass_classification_with_the_MNIST_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsi6YOieEow6"
      },
      "outputs": [],
      "source": [
        "# Handwritten Digit Recognition Example\n",
        "# This code demonstrates multiclass classification with the MNIST dataset\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Load the MNIST dataset (a smaller subset for quicker execution)\n",
        "# This dataset contains images of handwritten digits (0-9)\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False, parser='auto')\n",
        "X = X / 255.0  # Scale pixel values to between 0 and 1\n",
        "\n",
        "# To make the example run faster, let's use a smaller subset\n",
        "# In a real project, you might use all the data\n",
        "n_samples = 10000  # Using only 10,000 samples instead of all 70,000\n",
        "X_sample = X[:n_samples]\n",
        "y_sample = y[:n_samples]\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_sample, y_sample, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
        "\n",
        "# 3. Visualize some examples of the digits\n",
        "def plot_digits(X, y, indices):\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for i, index in enumerate(indices):\n",
        "        axes[i].imshow(X[index].reshape(28, 28), cmap='gray')\n",
        "        axes[i].set_title(f\"Label: {y[index]}\")\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Show 10 random examples\n",
        "random_indices = np.random.choice(len(X_train), size=10, replace=False)\n",
        "plot_digits(X_train, y_train, random_indices)\n",
        "\n",
        "# 4. Train a simple model (Softmax Regression)\n",
        "# For a real project, you might use neural networks which often perform better\n",
        "# But softmax regression is simpler to understand\n",
        "print(\"Training softmax regression model...\")\n",
        "# Using 'multinomial' option enables softmax regression\n",
        "softmax_model = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n",
        "                                   C=10, max_iter=100, random_state=42)\n",
        "softmax_model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Evaluate the model\n",
        "y_pred = softmax_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Softmax model accuracy: {accuracy:.4f} or {accuracy*100:.1f}%\")\n",
        "\n",
        "# 6. Visualize the confusion matrix to see where the model makes mistakes\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix for Digit Classification')\n",
        "plt.show()\n",
        "\n",
        "# 7. Let's also try a simple neural network for comparison\n",
        "print(\"\\nTraining a simple neural network...\")\n",
        "nn_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=20, random_state=42)\n",
        "nn_model.fit(X_train, y_train)\n",
        "\n",
        "nn_pred = nn_model.predict(X_test)\n",
        "nn_accuracy = accuracy_score(y_test, nn_pred)\n",
        "print(f\"Neural network accuracy: {nn_accuracy:.4f} or {nn_accuracy*100:.1f}%\")\n",
        "\n",
        "# 8. Visualize some predictions from the softmax model\n",
        "def plot_predictions(X, y_true, y_pred, indices):\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for i, index in enumerate(indices):\n",
        "        axes[i].imshow(X[index].reshape(28, 28), cmap='gray')\n",
        "        color = 'green' if y_pred[index] == y_true[index] else 'red'\n",
        "        axes[i].set_title(f\"True: {y_true[index]}, Pred: {y_pred[index]}\", color=color)\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Show some examples with their predictions\n",
        "random_test_indices = np.random.choice(len(X_test), size=10, replace=False)\n",
        "plot_predictions(X_test, y_test, y_pred, random_test_indices)\n",
        "\n",
        "# 9. Show probability distributions for a few examples\n",
        "def plot_probabilities(X, y_true, model, indices):\n",
        "    probs = model.predict_proba(X[indices])\n",
        "\n",
        "    fig, axes = plt.subplots(len(indices), 2, figsize=(12, 3*len(indices)))\n",
        "\n",
        "    for i, idx in enumerate(range(len(indices))):\n",
        "        # Plot the digit\n",
        "        axes[i, 0].imshow(X[indices[idx]].reshape(28, 28), cmap='gray')\n",
        "        axes[i, 0].set_title(f\"Digit: {y_true[indices[idx]]}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # Plot the probability distribution\n",
        "        axes[i, 1].bar(range(10), probs[idx])\n",
        "        axes[i, 1].set_xticks(range(10))\n",
        "        axes[i, 1].set_xlabel('Digit Class')\n",
        "        axes[i, 1].set_ylabel('Probability')\n",
        "        axes[i, 1].set_ylim(0, 1)\n",
        "        axes[i, 1].set_title('Class Probabilities')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Show probabilities for 5 examples\n",
        "plot_indices = random_test_indices[:5]\n",
        "plot_probabilities(X_test, y_test, softmax_model, plot_indices)\n"
      ]
    }
  ]
}