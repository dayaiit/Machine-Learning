{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9ErfkUEOyyBo33lp8ID0A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dayaiit/Machine-Learning/blob/main/L12_Simple_Computation_Graph_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PJuxtZJGtnC",
        "outputId": "a64a469a-0d83-41dd-eedc-84946544475c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = 2\n",
            "x = -2\n",
            "b = 8\n",
            "y = 2\n",
            "c = -4\n",
            "a = 4\n",
            "d = 2\n",
            "J = 2.0\n",
            "\n",
            "--- Forward pass complete ---\n",
            "\n",
            "dJ/dw = -4.0\n",
            "dJ/dx = 4.0\n",
            "dJ/db = 2.0\n",
            "dJ/dy = -2.0\n",
            "dJ/dc = 2.0\n",
            "dJ/da = 2.0\n",
            "dJ/dd = 2.0\n",
            "dJ/dJ = 1.0\n",
            "\n",
            "--- Verification ---\n",
            "Original J = 2.0\n",
            "J with w+epsilon = 1.9960020000000005\n",
            "Approximate gradient dJ/dw = -3.997999999999502\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ComputationNode:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.value = None\n",
        "        self.grad = None  # Will store the gradient (derivative)\n",
        "        self.parents = []  # Nodes that feed into this node\n",
        "        self.operation = None  # Function to compute this node's value\n",
        "        self.gradient_operation = None  # Function to compute gradients for parents\n",
        "\n",
        "    def forward(self):\n",
        "        # Compute this node's value based on its operation and parents\n",
        "        if self.operation:\n",
        "            self.value = self.operation()\n",
        "        return self.value\n",
        "\n",
        "    def backward(self, grad=1.0):\n",
        "        # This gradient represents how much the final output changes with respect to this node\n",
        "        self.grad = grad\n",
        "\n",
        "        # If this node has a gradient operation, use it to compute gradients for its parents\n",
        "        if self.gradient_operation:\n",
        "            self.gradient_operation(grad)\n",
        "\n",
        "# Let's implement our simple neural network example\n",
        "def create_simple_network(w=2, b=8, x=-2, y=2):\n",
        "    # Create nodes\n",
        "    w_node = ComputationNode(\"w\")\n",
        "    w_node.value = w\n",
        "\n",
        "    x_node = ComputationNode(\"x\")\n",
        "    x_node.value = x\n",
        "\n",
        "    b_node = ComputationNode(\"b\")\n",
        "    b_node.value = b\n",
        "\n",
        "    y_node = ComputationNode(\"y\")\n",
        "    y_node.value = y\n",
        "\n",
        "    # c = w * x\n",
        "    c_node = ComputationNode(\"c\")\n",
        "    c_node.parents = [w_node, x_node]\n",
        "    c_node.operation = lambda: w_node.value * x_node.value\n",
        "    c_node.gradient_operation = lambda grad: (\n",
        "        w_node.backward(grad * x_node.value),  # dC/dw = x\n",
        "        x_node.backward(grad * w_node.value)   # dC/dx = w\n",
        "    )\n",
        "\n",
        "    # a = c + b\n",
        "    a_node = ComputationNode(\"a\")\n",
        "    a_node.parents = [c_node, b_node]\n",
        "    a_node.operation = lambda: c_node.value + b_node.value\n",
        "    a_node.gradient_operation = lambda grad: (\n",
        "        c_node.backward(grad * 1),  # dA/dC = 1\n",
        "        b_node.backward(grad * 1)   # dA/dB = 1\n",
        "    )\n",
        "\n",
        "    # d = a - y\n",
        "    d_node = ComputationNode(\"d\")\n",
        "    d_node.parents = [a_node, y_node]\n",
        "    d_node.operation = lambda: a_node.value - y_node.value\n",
        "    d_node.gradient_operation = lambda grad: (\n",
        "        a_node.backward(grad * 1),   # dD/dA = 1\n",
        "        y_node.backward(grad * -1)   # dD/dY = -1\n",
        "    )\n",
        "\n",
        "    # J = 0.5 * d^2\n",
        "    j_node = ComputationNode(\"J\")\n",
        "    j_node.parents = [d_node]\n",
        "    j_node.operation = lambda: 0.5 * d_node.value**2\n",
        "    j_node.gradient_operation = lambda grad: (\n",
        "        d_node.backward(grad * d_node.value)  # dJ/dD = d\n",
        "    )\n",
        "\n",
        "    # Store all nodes in a list for easy access\n",
        "    nodes = [w_node, x_node, b_node, y_node, c_node, a_node, d_node, j_node]\n",
        "\n",
        "    return nodes\n",
        "\n",
        "# Create the network\n",
        "nodes = create_simple_network()\n",
        "\n",
        "# Forward pass - calculate the cost J\n",
        "for node in nodes:\n",
        "    node.forward()\n",
        "    print(f\"{node.name} = {node.value}\")\n",
        "\n",
        "print(\"\\n--- Forward pass complete ---\\n\")\n",
        "\n",
        "# Backward pass - calculate all gradients\n",
        "nodes[-1].backward()  # Start backpropagation from the final node (J)\n",
        "\n",
        "# Print gradients\n",
        "for node in nodes:\n",
        "    print(f\"dJ/d{node.name} = {node.grad}\")\n",
        "\n",
        "print(\"\\n--- Verification ---\")\n",
        "# Verify using the direct formula\n",
        "w, x, b, y = 2, -2, 8, 2\n",
        "J_original = 0.5 * ((w * x + b) - y)**2\n",
        "print(f\"Original J = {J_original}\")\n",
        "\n",
        "# Calculate J with a small change in w\n",
        "epsilon = 0.001\n",
        "J_new = 0.5 * (((w + epsilon) * x + b) - y)**2\n",
        "print(f\"J with w+epsilon = {J_new}\")\n",
        "print(f\"Approximate gradient dJ/dw = {(J_new - J_original) / epsilon}\")"
      ]
    }
  ]
}